<!DOCTYPE html>
<html lang="ja">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
	<meta name="description" content="Ishigame65の経緯と方針">
	<title>AIでカメを検出する</title>
	<link rel="shortcut icon" href="img/kame.ico">
	<link rel="stylesheet" href="common.css">
	<link rel="stylesheet" href="popup.css">
</head>

<style>
.box {
	width: 100%;
	padding: 5px;
	border: 1px solid gray;
	overflow-x: scroll;
}
@media (max-width: 670px) {
	main video {
		width: 90%;
	}
	pre {
		font-size: 8pt;
	}
}
</style>

<body>

<header>
	<a href="index.html"><img src="img/logo.png" alt="Ishigame65"></a>
	<nav>
		<a href="index.html"><b>Top</b></a>
		<a href="intro.html">経緯と方針</a>
		<a href="menu.html">飼育環境</a>
		<a href="maintenance.html">日々の管理</a>
	</nav>
	<div class="clear"></div>
</header>

<main>
	<ul class="breadcrumb">
		<li><a href="index.html">Top</a></li>
		<li>AIでカメを検出する</li>
	</ul>
	<h2 class="title">AIでカメを検出する</h2>
	<!-- 検出映像 -->
	<div align="center">
	<video id="movie" src="img/igwyolo02as.mp4" controls muted autoplay playsinline loop></video>
	</div>

	<h3>はじめに（予備知識）</h3>
    <p class="text">
	<ul>
	<li>物体検出を行う有名なAI深層学習モデルにYOLOがあるが、YOLOには開発チームの異なる多くのバージョンが存在する。
	<div class="box">
	<pre>
2015  2016  2017  2018  2019  2020  2021  2022  2023  2024
+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----
  v1       v2      v3                                        Joseph Redmon氏
                               v4            v7       v9     Alexey Bochkovskiy氏、Chien-Yao Wang氏チーム
                                v5              v8           Ultralyticsチーム
                                            v6               Meituanチーム
                                                        v10  精華大学チーム
	</pre>
	</div>
		<ul>
		<li>近年は主にChien-Yao Wang氏チームとUltralyticsチームが最新バージョンの公開を競っている印象がある。
		<li>性能と使い勝手の両方において先行バージョンの良いところを取り入れる形で進化しているため、基本的に新しいバージョンほど高性能で使い勝手が良いが、
		実用的な観点では、2020年のYOLO v5以降は使い勝手が似ており、YOLOのバージョンの違いよりも用意する学習データの違いが最終的な検出性能を左右する。
		<li>最近のYOLOは、当初からの物体検出にとどまらず、対象物の領域検出や追跡またヒトの姿勢推定などもサポートしている。
		</ul>
	<li>歴代のYOLOには、MicrosoftのCOCOデータセットを学習させた学習済モデルが付属しているが、COCOデータセットにカメが含まれていないため、
	自前でカメを学習させたモデルを作成しない限り、カメを検出することができない。
	試しにCOCO学習済モデルで検出させるとイシガメを「bird」と誤って検出した。
	<li>ここでは我が家のイシガメを学習させた物体検出モデルを作成して検出してみる。もしかしたらAIでカメを検出した世界初の事例かもしれない。
	<li>使用するAIモデルは、2023年にUltralyticsチームが公開したYOLO v8。
		<ul>
		<li>YOLO v8は<a href="https://github.com/ultralytics/ultralytics">Ultralytics版YOLO公式サイト</a>（2024年9月時点）の説明にしたがってインストール・学習（訓練）・検出（推論）する。
		</ul>
	</ul>
	</p>
	
	<h3>物体検出AIモデルの準備</h3>
	<div class="indent">
	<dl>
	<dt><img src="img/ai_ba2.png">
	<dd>
		<ul>
		<li>物体検出を行うAI深層学習モデル「YOLO」を準備する。
		<li>YOLOはカメを学習していない脳（神経回路網）モデルのため、カメを初めて見る赤ちゃんと同じでそれを「カメ」と認識することができない。
		</ul>
	</dl>
	</div>
	<h4>YOLO v8のインストール方法</h4>
    <p class="text">
	<ul>
	<li>YOLO v8をインストールする。コマンド1発でインストールできる。
	<div class="box">
	<pre>
pip install ultralytics
	</pre>
	</div>
		<ul>
		<li>2024年9月時点ではYOLO ver8.2.86がインストールされた。
		<li>必要に応じてPython仮想環境およびPyTorch環境（CUDAを含む）を用意してからインストールする。
		</ul>
	</ul>
	</p>

	<h3>学習データの準備</h3>
	<div class="indent">
	<dl>
	<dt><img src="img/ai_ba1.png">
	<dd>
		<ul>
		<li>どれがカメであるかを学習するためのデータ（画像とラベル）を準備する。
		<li>撮影した一枚一枚の画像に対して、画像中のどこにカメが映っているかを指定して、ラベルデータを作成する。この作業をアノテーションと呼ぶ。
		</ul>
	</dl>
	</div>
	<h4>画像とラベルを準備する方法</h4>
    <p class="text">
	<ul>
	<li>撮影映像からフレーム画像を取り出し、対象物の映っているフレームを選別する。
	<a href="https://phototools65.pages.dev/">PhotoTools65</a>の[VideoFrames]は映像からFPS値を指定して取り出したフレーム画像を連番ファイル名で保存できる。<br>
	<img src="img/vframes1.png" class="img400">
	<li>選別したフレーム画像から学習用画像を切り出しサイズを調整する。YOLOの学習用画像は640x640サイズにしておくのが好都合なので、
	フレーム画像から対象物の映っている正方形領域を切り出し、切り出した画像を640x640サイズに調整する。
	<a href="https://phototools65.pages.dev/">PhotoTools65</a>の[PhotoCrop]は写真または映像を指定領域で切り抜くことができるが、写真切り抜き時に元画像に上書き保存できるため効率的に切り出せる。
	また、<a href="https://phototools65.pages.dev/">PhotoTools65</a>の[MultiPhotoResize]は、複数写真を指定サイズに収まるように一括でリサイズできる。<br>
	<img src="img/pcrop1.png" class="img400">
	<img src="img/mpresize1.png" class="img250">
	<li>物体検出用のアノテーションを実行する。
	専用のアノテーションツールを利用して、画像ごとに対象物の名前（実際には対象物番号）と矩形座標が書かれたラベルデータを作成する。
	多くのアノテーションツールがあり好みで選べば良いが、YOLO形式のラベルデータを作成できるツールが便利である。
	<li>画像とラベルを、学習用と検証用におおむね8:2の比率で分けて、以下の例に示すようなフォルダ構成で格納する。
	画像格納フォルダパスのimagesをlabelsに置き換えるとラベル格納フォルダパスになるフォルダ構成になっていれば良い。
	<div class="box">
	<pre>
C:/dev/yolo/data/
 +- images/
 |   +- train/	<- 学習用画像格納フォルダ	C:\dev\yolo\data\images\train\
 |   +- val/	<- 検証用画像格納フォルダ	C:\dev\yolo\data\images\val\
 +- labels/
     +- train/	<- 学習用ラベル格納フォルダ	C:\dev\yolo\data\labels\train\
     +- val/	<- 検証用ラベル格納フォルダ	C:\dev\yolo\data\labels\val\
	</pre>
	</div>
	<li>学習用の設定ファイル[mydata.yaml]をテキストエディタで作成する（以下は例）。
	<div class="box">
	<pre>
path: C:\dev\yolo\data				# train,val共通パス
train: images\train				# 学習用画像フォルダ
val:   images\val				# 検証用画像フォルダ
nc: 4						# クラス数（対象物の数）
names: ['turtle','fish','shrimp','snail']	# クラス名（対象物の名前）
	</pre>
	</div>
	</ul>
	</p>

	<h3>学習（訓練）</h3>
	<div class="indent">
	<dl>
	<dt><img src="img/ai_ba3.png">
	<dd>
		<ul>
		<li>用意したデータ（画像＋ラベル）を使って脳（神経回路網）モデルを繰り返し訓練（学習）する。
		<li>繰り返し訓練することにより、神経回路網のシナプス結合の強さ（重み）が徐々に調整され、入力画像のどこにカメが映っているかを認識し出力できるようになる。
		</ul>
	</dl>
	</div>
	<h4>用意したデータを使って訓練する方法</h4>
    <p class="text">
	<ul>
	<li>テキストエディタで訓練スクリプト[train.py]を作成する。
	<div class="box">
	<pre>
from ultralytics import YOLO
if __name__ == '__main__':
	model = YOLO("yolov8n.pt")
	results = model.train(data='mydata.yaml', epochs=100, imgsz=640)
	</pre>
	</div>
		<ul>
		<li>YOLO v8にはサイズおよび精度の異なる5種類のモデルn,s,m,l,xがあり、この順にサイズが大きくなり処理時間が長くなる。ここでは最も小さいnモデル（yolov8n.pt）をもとに学習している。
		他のモデル（yolov8s.pt、yolov8m.ptなど）を指定しても良い。
		<li>PC環境によっては、batchやworkersの値を明示的に指定する必要がある（例、"batch=8, workers=4"）。
		</ul>
	<li>訓練スクリプト[train.py]を実行する。
	<div class="box">
	<pre>
python train.py
	</pre>
	</div>
		<ul>
		<li>指定したモデルが無い場合は最初にダウンロードされる。yolov8n.ptは6.24MB、yolov8s.ptは21.5MB、yolov8m.ptは49.7MB。
		</ul>
	<li>学習結果が runs\detect\train フォルダに格納される。
		<ul>
		<li>weights\last.ptが最終の学習モデル（重みデータ）で、weights\best.ptが最も精度が良かった学習モデル（重みデータ）である。
		<li>学習状況を可視化した図 results.png が保存されている。例を示す。<br>
		<img src="img/igaitrainresult1.png" width="100%">
			<ul>
			<li>lossが下がって落ち着けば学習が進んで収束したことを意味し、mAPは推論精度で学習モデルの性能を表す。
			<li>result.png以外にもAI分類モデルの性能評価指標の推移曲線が多く保存されている。
			</ul>
		</ul>
	</ul>
	</p>

	<h3>検出（推論）</h3>
	<div class="indent">
	<dl>
	<dt><img src="img/ai_ba4.png">
	<dd>
		<ul>
		<li>学習した脳（神経回路網）モデルを使用してカメを検出（推論）する。
		<li>すでに入力画像のどこにカメが映っているかを認識し出力できる神経回路網になっているため、カメを検出（入力画像のどこにカメがいるかを推論）することができる。
		</ul>
	</dl>
	</div>
	<h4>映像からカメを検出する方法</h4>
    <p class="text">
	<ul>
	<li>テキストエディタで推論スクリプト[detect.py]を作成する。
	<div class="box">
	<pre>
from ultralytics import YOLO
model = YOLO('./runs/detect/train/weights/best.pt')
results = model('movie.mp4', save=True, conf=0.5) 
	</pre>
	</div>
		<ul>
		<li>最も精度が良かった学習モデル best.pt を使用して、映像ファイル'movie.mp4'から検出（推論）し、信頼度スコア0.5以上の検出映像を出力する例。
		</ul>
	<li>推論スクリプト[detect.py]を実行する。
	<div class="box">
	<pre>
python detect.py
	</pre>
	</div>
	<li>検出映像が runs\detect\predict フォルダに元映像と同じファイル名で格納される。
	</ul>
	</p>
	<h4>画像からカメを検出する方法</h4>
    <p class="text">
	<ul>
	<li>テキストエディタで推論スクリプト[detect.py]を作成する。
	<div class="box">
	<pre>
from ultralytics import YOLO
model = YOLO('./runs/detect/train/weights/best.pt')
results = model('img_folder', save=True, save_txt=True, conf=0.5) 
	</pre>
	</div>
		<ul>
		<li>指定フォルダ'img_folder'にある画像から検出し、信頼度スコア0.5以上の検出画像と検出ラベルを出力する例。
		<li>'save_txt=True'オプション付加により、検出ラベル（対象物番号と矩形座標）をファイル出力する。
		<li>ここでは検出対象として画像が格納されたフォルダを指定しているが、一枚の画像ファイルを指定することもできる。
		</ul>
	<li>推論スクリプト[detect.py]を実行する。
	<div class="box">
	<pre>
python detect.py
	</pre>
	</div>
	<li>検出画像が runs\detect\predict フォルダに元画像と同じファイル名で格納される。
	<li>検出ラベルが runs\detect\predict\labels フォルダに画像ファイル名＋拡張子txtのファイル名で格納される。
	アノテーションで作成するラベルデータと同じ形式で保存されるため、この検出ラベルをアノテーションデータとして利用できる（いわゆる自動アノテーションだが、検出できるモノをアノテーションする意味は？？）。
	</ul>
	</p>

	<h3>検出性能の比較</h3>
	<!--
	2時間25分7秒の撮影映像（1920x1980画素、29.97fps）から5秒ごとのフレーム画像を抽出し、
	カメが映り込んでいる880フレームを選び、
	-->
	<div class="text">
	カメが延べ1999回映り込んでいる880画像に対して、条件を変えて検出性能を比較した結果を示す。
	<ul>
		<li>検出数は画像に映り込んでいるカメを正しく検出した回数、誤検出数は画像中のカメではないモノをカメとして検出した回数。
		<li>検出数を1999で割ることで検出率を算出できるが、この値は意味を持たない。
		880画像にはカメがごく一部しか映り込んでおらず到底検出できないであろう画像を多く含んでいるが、画像の選び方を少し変えれば簡単に検出率を100％に近づけることができるからである。
		数値自体にあまり意味が無くても、条件を変えた時の検出性能を比較する目的には有効に使える。
	</ul>
	</div>

	<h4>Epoch数（学習回数）による性能比較</h4>
	<img src="img/graph_y8nepoch.png">
	<div class="text">
	<ul>
	<li>Epoch数が多いほど良いわけではない。学習データの品質、性能評価指標の推移曲線、最終的には用途に応じた検出数と誤検出数のバランスを踏まえて適切なEcoch数を選択する。
	<li>学習時間は学習画像数やEpoch数に比例する。本実験の学習画像は363枚。
	</ul>
	</div>

	<h4>モデルサイズによる性能比較</h4>
	<img src="img/graph_y8msize.png">
	<div class="text">
	<ul>
	<li>大きいモデルの方が高性能になるが、大きいモデルで学習するためには高性能のGPUマシンが必要になる。
	<li>低性能GPUマシンでの学習時間実測値は、YOLOv8n（Nano）が12分、YOLOv8s（Small）が20分、YOLOv8m（Medium）が236分。
	<li>学習時間は学習画像数やEpoch数に比例する。本実験の学習画像は363枚、Epoch数は100。
	</ul>
	</div>

	<!--
	AIの検出性能を向上させる方法
	・検出性能を評価する方法
	・学習画像数による性能比較
	-->

</main>

<footer>
	<div class="footer-left">
	<a href="https://www.instagram.com/ishigame65/"><img src="img/insta60.png" alt="Instagram" width=32 height=32></a>
	</div>
	<div class="footer-center"><a href="keeper.html">飼育係</a><a href="sitemap.html">サイトマップ</a>
	<a href="search.html">検索</a>
	</div>
	<div class="footer-right"><a href="copyright.html">&copy; 2024 Ishigame65.</a></div>
	<div class="clear"></div>
</footer>

<script src="popup.js"></script>

</body>
</html>
