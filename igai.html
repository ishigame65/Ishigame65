<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="css/style.css">    
	<meta name="description" content="AIでカメを検出する">
	<link rel="shortcut icon" href="img/kame.ico">
	<title>AIでカメを検出する</title>
</head>
<style>
    .img250 {
        border-radius:4px;
    }
    .img300 {
        border-radius:4px;
    }
    .img400 {
        border-radius:4px;
    }
    .img500 {
        border-radius:4px;
    }
    main img {
        margin-right: 1em;
        margin-bottom: 0.5em;
        max-width: 90%;
    }
    .box {
        max-width: 90%;
        padding: 5px;
        border: 1px solid gray;
        overflow-x: scroll;
    }
    .box2 {
        max-width: 90%;
        padding: 5px;
        border: 1px solid gray;
    }
    .box3 {
        padding: 15px;
        color: gray;
    }
    .fukidashi {
        color: gray;
        position: relative;
        top: -10px;
        left: 35px;
        font-size: small;
        opacity: 0; transition:opacity 4s;
    }
    .box3:hover + .fukidashi {
        opacity: 1; transition:opacity 2s;
    }
    @media (max-width: 670px) {
        main video {
            /* width: 90%; */
            max-width: 90%;
        }
        pre {
            font-size: 8pt;
        }
    }
</style>
<body>
 
<!-- input・label・ドロワーメニューはbody直下 -->
<input id="check_input" type="checkbox">
<label id="menu_btn" for="check_input"> <!-- ハンバーガーボタン部分 -->
    <span></span><div id="title">MENU</div>
</label>
<label id="drawer_back" for="check_input"></label> <!-- ドロワーメニューの背景部分 -->

<div id="menu_cont"> <!-- ドロワーメニュー本体部分 -->
    <ul>
        <li><a href="index.html">TOP</a></li>
        <li><a href="intro.html">経緯と方針</a></li>
        <li><a href="menu.html">飼育環境</a></li>
        <li><a href="maintenance.html">日々の管理</a></li>
        <li><a href="baby.html">成長記録</a></li>
        <li><a href="entertainment.html">遊び</a></li>
        <li><a href="keeper.html">飼育係</a></li>
    </ul>
    <ul id="menu2nd">
        <li>&nbsp;</li>
        <li><a href="https://www.instagram.com/ishigame65/">Instagram<img src="img/insta60.png" alt="" id="instaicon"></a></li>
        <li>&nbsp;</li>
        <li><a href="links.html">リンク</a></li>
        <li><a href="copyright.html">サイトポリシー</a></li>
        <li><a href="sitemap.html">サイトマップ</a></li>
        <li><a href="search.html">サイト内検索</a></li>
    </ul>
</div>

<header>
    <h1><a href="index.html"><img id="logo" src="img/logo.png" alt="Ishigame65"></a></h1>
</header>

<div id="wrapper"> <!-- コンテンツ部分をdivで囲む　input要素と同じ階層に配置する -->
    <main> <!-- メインコンテンツ -->
        <div class="breadcrumb">
            <ul>
                <li><a href="index.html">Top</a></li>
                <li><a href="entertainment.html">遊び</a></li>
                <li>AIでカメを検出する</li>
            </ul>
        </div> <!-- breadcrumb -->
        <h2>AIでカメを検出する</h2>
        <section class="indent">            
            <!-- 検出映像 -->
            <video id="movie" src="img/igwyolo02as.mp4" controls muted autoplay playsinline loop></video>
        </section>
    
        <h3>はじめに（予備知識）</h3>
        <section class="indent">            
            <ul>
                <li>物体検出を行う有名なAI深層学習モデルにYOLOがあるが、YOLOには開発チームの異なる多くのバージョンが存在する。
                <div class="box indent">
                    <pre>
2015  2016  2017  2018  2019  2020  2021  2022  2023  2024
+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----
   v1       v2      v3                                       Joseph Redmon氏
                                v4           v7       v9     Alexey Bochkovskiy氏、Chien-Yao Wang氏チーム
                                v5              v8           Ultralyticsチーム
                                            v6               Meituanチーム
                                                        v10  精華大学チーム
                    </pre>
                </div>
                <ul class="indent">
                    <li>近年は主にChien-Yao Wang氏チームとUltralyticsチームが最新バージョンの公開を競っている印象がある。
                    <li>性能と使い勝手の両方において先行バージョンの良いところを取り入れる形で進化しているため、基本的に新しいバージョンほど高性能で使い勝手が良いが、
                    実用的な観点では、2020年のYOLO v5以降は使い勝手が似ており、YOLOのバージョンの違いよりも用意する学習データの違いが最終的な検出性能を左右する。
                    <li>最近のYOLOは、当初からの物体検出にとどまらず、対象物の領域検出や追跡またヒトの姿勢推定などもサポートしている。
                </ul>
                <li>歴代のYOLOには、MicrosoftのCOCOデータセットを学習させた学習済モデルが付属しているが、COCOデータセットにカメが含まれていないため、
                自前でカメを学習させたモデルを作成しない限り、カメを検出することができない。試しにCOCO学習済モデルで検出させるとイシガメを「bird」と誤って検出した。
                <li>ここでは我が家のイシガメを学習させた物体検出モデルを作成して検出してみる。
                <li>使用するAIモデルは、2023年にUltralyticsチームが公開したYOLO v8の最新版ver8.2（2024年9月時点）。
                <ul class="indent">
                    <li>YOLO v8は<a href="https://github.com/ultralytics/ultralytics">Ultralytics版YOLO公式サイト</a>（2024年9月時点）の説明にしたがってインストール・学習（訓練）・検出（推論）する。
                </ul>
            </ul>
        </section>
        
        <h3>物体検出AIモデルの準備</h3>
        <section class="indent">            
            <dl>
            <dt><img src="img/ai_ba2.png">
            <dd>
                <ul>
                <li>物体検出を行うAI深層学習モデル「YOLO」を準備する。
                <li>YOLOはカメを学習していない脳（神経回路網）モデルのため、カメを初めて見る赤ちゃんと同じでそれを「カメ」と認識することができない。
                </ul>
            </dl>
        </section>

        <h4>YOLO v8のインストール方法</h4>
        <section class="indent">            
            <ul>
                <li>YOLO v8をインストールする。コマンド1発でインストールできる。
                <div class="box2 indent">
                    <pre>
pip install ultralytics
                    </pre>
                </div>
                <ul class="indent">
                    <li>2024年9月時点ではYOLO ver8.2.86がインストールされた。
                    <li>必要に応じてPython仮想環境およびPyTorch環境（CUDAを含む）を用意してからインストールする。
                </ul>
            </ul>
        </section>

        
        <h3>学習データの準備</h3>
        <section class="indent">
            <dl>
            <dt><img src="img/ai_ba1.png">
            <dd>
                <ul>
                <li>どれがカメであるかを学習するためのデータ（画像とラベル）を準備する。
                <li>撮影した一枚一枚の画像に対して、画像中のどこにカメが映っているかを指定して、ラベルデータを作成する。この作業をアノテーションと呼ぶ。
                </ul>
            </dl>
        </section>

        <h4>画像とラベルを準備する方法</h4>
        <section class="indent">            
            <ul>
                <li>撮影映像からフレーム画像を取り出し、対象物の映っているフレームを選別する。
                <a href="https://phototools65.pages.dev/">PhotoTools65</a>の[VideoFrames]は映像からFPS値を指定して取り出したフレーム画像を連番ファイル名で保存できる。<br>
                <img src="img/vframes1.png" class="img400">
                <li>選別したフレーム画像から学習用画像を切り出しサイズを調整する。YOLOの学習用画像は640x640サイズにしておくのが好都合なので、
                フレーム画像から対象物の映っている正方形領域を切り出し、切り出した画像を640x640サイズに調整する。
                <a href="https://phototools65.pages.dev/">PhotoTools65</a>の[PhotoCrop]は写真または映像を指定領域で切り抜くことができるが、写真切り抜き時に元画像に上書き保存できるため効率的に切り出せる。
                また、<a href="https://phototools65.pages.dev/">PhotoTools65</a>の[MultiPhotoResize]は、複数写真を指定サイズに収まるように一括でリサイズできる。<br>
                <img src="img/pcrop1.png" class="img400">　　
                <img src="img/mpresize1.png" class="img250">
                <li>物体検出用のアノテーションを実行する。
                専用のアノテーションツールを利用して、画像ごとに対象物の名前（実際には対象物番号）と矩形座標が書かれたラベルデータを作成する。
                多くのアノテーションツールがあり好みで選べば良いが、YOLO形式のラベルデータを作成できるツールが便利である。
                <li>画像とラベルを、学習用と学習状況検証用におおむね8:2の比率で分けて、以下の例に示すようなフォルダ構成で格納する。
                画像格納フォルダパスのimagesをlabelsに置き換えるとラベル格納フォルダパスになるフォルダ構成になっていれば良い。
                <div class="box indent">
                    <pre>
C:/dev/yolo/data/
+- images/
|   +- train/	<- 学習用画像格納フォルダ		C:\dev\yolo\data\images\train\
|   +- val/	<- 学習状況検証用画像格納フォルダ	C:\dev\yolo\data\images\val\
+- labels/
    +- train/	<- 学習用ラベル格納フォルダ		C:\dev\yolo\data\labels\train\
    +- val/	<- 学習状況検証用ラベル格納フォルダ	C:\dev\yolo\data\labels\val\
                    </pre>
                </div>
                <li>学習用の設定ファイル[mydata.yaml]をテキストエディタで作成する（以下は例）。
                <div class="box indent">
                    <pre>
path: C:\dev\yolo\data				# train,val共通パス
train: images\train				# 学習用画像フォルダ
val:   images\val				# 学習状況検証用画像フォルダ
nc: 4						# クラス数（対象物の数）
names: ['turtle','fish','shrimp','snail']	# クラス名（対象物の名前）
                    </pre>
                </div>
            </ul>
        </section>

        <h3>学習（訓練）</h3>
        <section class="indent">            
            <dl>
            <dt><img src="img/ai_ba3.png">
            <dd>
                <ul>
                <li>用意したデータ（画像＋ラベル）を使って脳（神経回路網）モデルを繰り返し訓練（学習）する。
                <li>繰り返し訓練することにより、神経回路網のシナプス結合の強さ（重み）が徐々に調整され、入力画像のどこにカメが映っているかを認識し出力できるようになる。
                </ul>
            </dl>
        </section>

        <h4>用意したデータを使って訓練する方法</h4>
        <section class="indent">            
            <ul>
                <li>テキストエディタで訓練スクリプト[train.py]を作成する。
                <div class="box indent">
                    <pre>
from ultralytics import YOLO
if __name__ == '__main__':
    model = YOLO("yolov8n.pt")
    results = model.train(data='mydata.yaml', epochs=100, imgsz=640)
                    </pre>
                </div>
                <ul class="indent">
                    <li>YOLO v8にはサイズおよび精度の異なる5種類のモデルn,s,m,l,xがあり、この順にサイズが大きくなり処理時間が長くなる。
                    ここでは最も小さいnモデル（yolov8n.pt）をもとに転移学習している。yolov8s.pt、yolov8m.ptなど適宜使用するモデルを指定する。
                    <li>PC環境によっては、batchやworkersの値を明示的に指定する必要がある（例、"batch=8, workers=4"）。
                </ul>
                <li>訓練スクリプト[train.py]を実行する。
                <div class="box2 indent">
                    <pre>
python train.py
                    </pre>
                </div>
                <ul class="indent">
                    <li>指定したモデルが無い場合は最初にダウンロードされる。yolov8n.ptは6.24MB、yolov8s.ptは21.5MB、yolov8m.ptは49.7MB。
                </ul>
                <li>学習結果が runs\detect\ フォルダの train から始まる名前のフォルダに格納される。
                <ul class="indent">
                    <li>nameオプション指定（例：name="mytrain"）した場合は、指定した名前のフォルダに格納される。
                    <li>weights\last.ptが最終の学習モデル（重みデータ）で、weights\best.ptが最も精度が良かった学習モデル（重みデータ）である。
                    <li>学習状況を可視化した図 results.png が保存されている。例を示す。<br>
                    <!-- <img src="img/igaitrainresult1.png" width="100%"> -->
                    <img src="img/igaitrainresult1.png">
                    <ul class="indent">
                        <li>lossが下がって落ち着けば学習が進んで収束したことを意味し、mAPは推論精度で学習モデルの性能を表す。
                        <li>result.png以外にもAI分類モデルの性能評価指標の推移曲線が多く保存されている。
                    </ul>
                </ul>
            </ul>
        </section>
        
        <h3>検出（推論）</h3>
        <section class="indent">            
            <dl>
            <dt><img src="img/ai_ba4.png">
            <dd>
                <ul>
                    <li>学習した脳（神経回路網）モデルを使用してカメを検出（推論）する。
                    <li>すでに入力画像のどこにカメが映っているかを認識し出力できる神経回路網になっているため、カメを検出（入力画像のどこにカメがいるかを推論）できる。
                </ul>
            </dl>
        </section>

        <h4>映像からカメを検出する方法</h4>
        <section class="indent">            
            <ul>
                <li>テキストエディタで推論スクリプト[detect.py]を作成する。
                <div class="box indent">
                    <pre>
from ultralytics import YOLO
model = YOLO('./runs/detect/train/weights/best.pt')
results = model('movie.mp4', save=True, conf=0.5) 
                    </pre>
                </div>
                <ul class="indent">
                    <li>最も精度が良かった学習モデル best.pt を使用して、映像ファイル'movie.mp4'から検出（推論）し、信頼度スコア0.5以上の検出映像を出力する例。
                </ul>
                <li>推論スクリプト[detect.py]を実行する。
                <div class="box2 indent">
                    <pre>
python detect.py
                    </pre>
                </div>
                <li>検出映像が runs\detect\ フォルダの predict から始まる名前のフォルダに元映像と同じファイル名で格納される。
                <ul class="indent">
                    <li>nameオプション指定（例：name="mypredict"）した場合は、指定した名前のフォルダに格納される。
                </ul>
            </ul>
        </section>        
        <h4>画像からカメを検出する方法</h4>
        <section class="indent">
            <ul>
            <li>テキストエディタで推論スクリプト[detect.py]を作成する。
            <div class="box indent">
                <pre>
from ultralytics import YOLO
model = YOLO('./runs/detect/train/weights/best.pt')
results = model('img_folder', save=True, save_txt=True, conf=0.5) 
                </pre>
            </div>
            <ul class="indent">
                <li>指定フォルダ'img_folder'にある画像から検出し、信頼度スコア0.5以上の検出画像と検出ラベルを出力する例。
                <li>'save_txt=True'オプション付加により、検出ラベル（対象物番号と矩形座標）をファイル出力する。
                <li>ここでは検出対象として画像が格納されたフォルダを指定しているが、一枚の画像ファイルを指定することもできる。
            </ul>
            <li>推論スクリプト[detect.py]を実行する。
            <div class="box2 indent">
                <pre>
python detect.py
                </pre>
            </div>
            <li>検出画像が runs\detect\ フォルダの predict から始まる名前のフォルダに元画像と同じファイル名で格納される。
            <li>検出ラベルが runs\detect\predict\labels フォルダに画像ファイル名＋拡張子txtのファイル名で格納される。
            アノテーションで作成するラベルデータと同じ形式で保存されるため、この検出ラベルをアノテーションデータとして学習（訓練）に利用できる。
            いわゆる一つの自動アノテーションだが、検出できるモノをアノテーションする意味は？？
            <div class="indent">
                <div class="box3">
                    両手の鳴る音は知る。 片手の鳴る音はいかに？  　 → <a href="igaioptim.html">AI学習データ自動最適化による検出性能向上</a> につづく
                </div>
                <div class="fukidashi">
                    ナイン・ストーリーズ/J.D.Salinger（野崎孝訳）のエピグラフより
                </div>
                <!--分別しないこともまた分別・・かも-->
            </div>
            </ul>
        </section>
    
        <h3>検出性能の比較</h3>
        <section class="indent">
            2時間25分7秒の撮影映像（1920x1980画素、29.97fps）から5秒おきに抽出したフレーム画像の中でカメが映り込んでいる880画像に対して、
            複数の学習モデルで検出を行った結果を比較する。
            <ul>
                <li>検出数は画像に映り込んでいるカメを正しく検出した回数、誤検出数は画像中のカメではないモノをカメとして検出した回数。
                <li>検出数が多く誤検出数が少ない方が良い。最終的には用途に応じた検出数と誤検出数のバランスを踏まえて適切な条件を選択する。
            </ul>
        </section>
    
        <h4>Epoch数（学習回数）による性能比較</h4>
        <section class="indent">            
            <img src="img/graph_y8nepoch.png">
            <ul>
                <li>Epoch数が多いほど良いわけではない。学習データの品質および性能評価指標の推移曲線に基づいて適切なEcoch数を選択する。
                <li>本実験の学習画像は、撮影日や撮影位置の異なる複数の撮影映像から抽出した640x640画素サイズの454画像。
                <li>学習時間は使用機器性能・モデルサイズ・学習パラメータ値によって異なり、学習画像数やEpoch数に比例する。
            </ul>
        </section>
    
        <h4>モデルサイズによる性能比較</h4>
        <section class="indent">            
            <img src="img/graph_y8msize.png">
            <img src="img/graph_y8dtime.png">
            <ul>
                <li>大きいモデルの方が検出性能が高いが、大きいモデルで学習するためには高性能のGPUマシンが必要になる。
                <li>画像毎の検出時間はいずれも33ms以下であり、フルHD解像度で秒30コマのリアルタイム検出を実現できる速度である。
                <ul class="indent">
                    <li>なお、Yolo標準実装は画像と映像で検出解像度の扱いが異なり、映像の場合は入力映像解像度がそのまま検出解像度になるわけではない点に注意が必要。</li>
                </ul>
                <li>本実験の学習画像は、撮影日や撮影位置の異なる複数の撮影映像から抽出した640x640画素サイズの454画像。Epoch数は100。
                <li>学習時間は使用機器性能・モデルサイズ・学習パラメータ値によって異なり、学習画像数やEpoch数に比例する。
                低性能GPUマシンでの学習時間実測値は、YOLOv8n（Nano）が12分、YOLOv8s（Small）が20分、YOLOv8m（Medium）が236分。
            </ul>
        </section>
    
        <h3>関連記事</h3>
        <section class="indent">            
            <ul>
            <li><a href="igaiy8910.html">AIモデルYOLOv8・v9・v10の検出性能比較</a>
            <li><a href="igaioptim.html">AI学習データ自動最適化による検出性能向上</a>
            </ul>
        </section>

    </main>
    <footer> <!-- フッターコンテンツ -->
        <a href="https://www.instagram.com/ishigame65/"><img src="img/insta60.png" alt="" id="instaicon2"></a>
        <a href="#logo"><span class="arrow-up"></span></a>
    </footer>
</div>
    
</body>
</html>